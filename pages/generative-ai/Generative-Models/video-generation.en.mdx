# Vedio Generation

AI video generation utilizes artificial intelligence to automatically produce video content. By understanding user input, it generates visual and audio elements, offering an efficient way to create videos for various applications.

<br />
import Image from 'next/image'
import myGif from 'public/gif/aiVedio.gif'

<Image src={myGif} alt="my gif"/>

### Article & Papers

- [Make-A-Video](https://makeavideo.studio/) (2022) by Meta AI, is an AI system that generates high-quality video clips from text prompts. It leverages recent advancements in text-to-image generation technology and utilizes publicly available datasets for transparency, it empowers users to create content easily and can create videos from images or remix existing ones. [(paper)](https://arxiv.org/abs/2209.14792)

- [Imagen Video: High Definition Video Generation with Diffusion Models](https://arxiv.org/abs/2210.02303) (2022) by Google, is a paper introducing a high-definition video generation method using diffusion models. The technique, based on sequential noise addition to latent representations, achieves top results in video synthesis benchmarks. [(website)](https://imagen.research.google/video/)

### Reference

- [Papers with Code](https://paperswithcode.com/): Collection of papers and benchmarks related to [video generation](https://paperswithcode.com/task/video-generation), [Text-to-Video Generation](https://paperswithcode.com/task/text-to-video-generation) and [Image to Video Generation](https://paperswithcode.com/task/image-to-video).

- [Camenduru's GitHub Video ML Papers](https://github.com/camenduru#-video-ml-papers): This collection comprises repositories containing video machine learning papers. It also encompasses projects related to text-to-video synthesis, diffusion models, and video retalking.

- [Camenduru's 3D Motion Papers](https://github.com/camenduru#-3d-motion-papers): This collection contains repositories on 3D motion papers, includes projects like MotionDiffuse, NIKI, PHALP, DWPose, 4D-Humans, vid2avatar, and PARE.